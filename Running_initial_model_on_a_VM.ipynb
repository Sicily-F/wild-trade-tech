{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO49er15Hu6jLaE2Dp2Qx3a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sicily-F/wild-trade-tech/blob/main/Running_initial_model_on_a_VM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running a machine learning model on a VM\n",
        "\n",
        "## Project introduction\n",
        "\n",
        "We have decided to start this project as part of a wider PhD project by Sicily Fiennes at the [University of Leeds](https://biologicalsciences.leeds.ac.uk/biological-sciences/pgr/1934/sicily-fiennes), under the provisional title of 'Opportunities for co-design of technology to address the Southeast Asian illegal bird trade'. This PhD project emerged from an earlier proof-of-concept for my Master's at the University of Kent (https://github.com/Sicily-F/cagedbirdID). Late last year, I was lucky enough to get an [AI For Earth Grant](https://www.microsoft.com/en-us/ai/ai-for-earth#:~:text=Program%20details.%20AI%20for%20Earth%20grants%20provide%20access,and%20cloud%20computing%20with%20your%20existing%20labeled%20dataset.) from Microsoft Azure, which gave me access to computing credits to run code on Azure machines. Another tutorial dictates our creation of the virtual machine, and how to access it.\n",
        "\n",
        "After logging onto the VM, the following was run in the command line of Windows PowerShell, which using an ssh key can connect you to your Azure VM.\n",
        "\n"
      ],
      "metadata": {
        "id": "sXKheq75BMmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Using the MegaDetector Model... again\n",
        "This current tutorial is merely to explain how we set Python up on our VM (the creation of which was explained in our earlier tutorial. Primarily, we have been concerned with the use of the MegaDetector model to crop images of birds that are found in wildlife marketplaces. Although the MegaDetector model has now been updated to [version 5](https://github.com/microsoft/CameraTraps), currently we have kept the implementation as it stands using version 4, though this will be updated in coming months. Taking inspiration from our earlier [work](https://github.com/Sicily-F/cagedbirdID/blob/main/6_Megadetector_code.ipynb), we sought to use the MegaDetector on an Azure VM.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3UIo6YV7BP1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We downloaded the requirements.txt from the [Docker image](https://github.com/sam-watts/tf-docker) which we had previously built; again this file needs updating in terms of some of the packages used, but it is still working. This file was downloaded from GitHub locally, and then transferred onto the data drive of our VM using the very convenient WinSCP program, which is basically another version of Windows file explorer and let's you easily copy files across.\n",
        "\n",
        "We then used Windows Powershell to remotely connect to our VM (instructions in previous tutorial). We can then run code directly from the command line in Windows Powershell, as you would on a local machine or when connecting to a high performance computer.\n",
        "\n",
        "Before running a machine learning model, we have to make sure that all of the [packages](https://github.com/sam-watts/tf-docker/blob/master/requirements.txt) that were used to run our models previously on a High Performance Computer (HPC, are now downloaded onto our new virtual machine.\n",
        "\n"
      ],
      "metadata": {
        "id": "noaxpPMadf2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The use of \n",
        "```\n",
        "./\n",
        "```\n",
        "allows us to specify the root directory of our VM.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P2a0IkWnZ1P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install â€“r ./requirements.txt "
      ],
      "metadata": {
        "id": "BoSNXR9rj0-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One caveat is that to enable a Microsoft Azure VM with GPU support is very challenging. We tried to add the NVIDIA GPU Driver extension but on the VM size standard_d2s_v3 not supported yet for installation, you can find more information on this here [though](https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/hpccompute-gpu-linux).\n",
        "\n",
        "What we did is, whilst working on the Azure VM, we only paid for the more powerful CPU whilst actually running the model.  Then when it's done, you can change the VM type to something cheaper while you do all the cropping, or do all the cropping locally.  \n",
        "\n",
        "We changed our VM from a Standard D2s v3 (2 vcpus, 8 GiB memory), to a D4s v3 size."
      ],
      "metadata": {
        "id": "1Bodd0yUabWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python3 ./cameratraps/detection/run_tf_detector.py md_v4.1.0.pb --image_dir ./balanced_museums/non_museums/asian_fairy_bluebird --crop "
      ],
      "metadata": {
        "id": "3x5ItNZmZw6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running a machine learning model\n",
        "In order to write the output of the terminal to file, we specify the creation of a new out.txt file, which is made in the root directory and constantly updated."
      ],
      "metadata": {
        "id": "KZMbb8fNpBk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python3 densenet201augre16.py &> out.txt "
      ],
      "metadata": {
        "id": "6k8nU6Sriha9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}